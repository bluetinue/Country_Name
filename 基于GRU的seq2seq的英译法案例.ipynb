{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "193mR9CddEDVBTJka6zFndtawvyxeVBx3",
      "authorship_tag": "ABX9TyNg+SCo3yKmQuBSHpfsByTg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluetinue/Country_Name/blob/main/%E5%9F%BA%E4%BA%8EGRU%E7%9A%84seq2seq%E7%9A%84%E8%8B%B1%E8%AF%91%E6%B3%95%E6%A1%88%E4%BE%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s4U2eD2SVwe",
        "outputId": "0df3301d-b692-4b2b-eadd-2051734711ec",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@title 导包\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 用于正则表达式\n",
        "import re\n",
        "# 用于构建网络结构和函数的torch工具包\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# torch中预定义的优化方法工具包\n",
        "import torch.optim as optim\n",
        "import time\n",
        "# 用于随机生成数据\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 全局变量\n",
        "\n",
        "#开始字符标注\n",
        "SOS_TOKEN = 0\n",
        "EOS_TOKEN = 1\n",
        "\n",
        "#最大句子长度\n",
        "MAX_LENGTH = 10\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/NLP/data/eng-fra-v2.txt\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "mE9rJIqSbVHp",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.cuda\n",
        "\n",
        "# 默认没有设置过cuda时,CUDA available结果是False,如果设置后是True\n",
        "print(\"CUDA是否可用:\", torch.cuda.is_available())\n",
        "print(\"CUDA版本:\", torch.version.cuda)\n",
        "print(\"PyTorch版本:\", torch.__version__)\n",
        "print(\"电脑上GPU数量:\", torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    \tprint(\"当前使用的GPU版本:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j3qQ5Ghphip",
        "outputId": "f7fb97e5-a741-4516-8cd9-eeef8eeadafc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA是否可用: False\n",
            "CUDA版本: 12.4\n",
            "PyTorch版本: 2.6.0+cu124\n",
            "电脑上GPU数量: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 文本清洗工具函数\n",
        "def normal2String(s):\n",
        "  s1 = s.lower().strip()\n",
        "  s2 = re.sub(r\"([.!?])\", r\" \\1 \", s1)\n",
        "  s3 = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s2)\n",
        "  return s3.strip()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1v6xnFkLTJYd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 对原始数据进行预处理\n",
        "#构建出[[英文,法文，.....的列表对]]\n",
        "def wash_data():\n",
        "  with open(data_path,\"r\",encoding=\"utf-8\") as fr:\n",
        "    lines = fr.readlines()\n",
        "  #列表推导式\n",
        "  my_pairs = [[ normal2String(s) for s in i.strip().split(\"\\t\")] for i in lines]\n",
        "\n",
        "  #初始化词表，默认有开始和结束分隔符和初始长度\n",
        "  english_word2index = {\"SOS\":0,\"EOS\":1}\n",
        "  english_word2index_n = 2\n",
        "  fres_word2index = {\"SOS\":0,\"EOS\":1}\n",
        "  fres_word2index_n = 2\n",
        "\n",
        "  #构造两个词表的word2index表\n",
        "  for pair in my_pairs:\n",
        "    for word in pair[0].split(\" \"):\n",
        "      if word not in english_word2index:\n",
        "        english_word2index[word] = len(english_word2index)\n",
        "        #english_word2index[word] = english_word2index_n\n",
        "        #english_word2index_n += 1\n",
        "\n",
        "    for word in pair[1].split(\" \"):\n",
        "      if word not in fres_word2index:\n",
        "        fres_word2index[word] = len(fres_word2index)\n",
        "        #fres_word2index[word] = fres_word2index_n\n",
        "        #fres_word2index_n += 1\n",
        "\n",
        "  #构造两个词表的index2word表\n",
        "  english_index2word = {v:k for k,v in english_word2index.items()}\n",
        "  fres_index2word = {v:k for k,v in fres_word2index.items()}\n",
        "  return english_word2index,english_index2word,\\\n",
        "    len(english_word2index),fres_word2index,fres_index2word,\\\n",
        "    len(fres_word2index),my_pairs"
      ],
      "metadata": {
        "id": "1auaNNPuc5OS",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_word2index, english_index2word,  english_word_n, french_word2index, french_index2word, french_word_n, my_pairs = wash_data()"
      ],
      "metadata": {
        "id": "9K1cxBUSvDxZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 构建数据源对象\n",
        "class SeqDataset(Dataset):\n",
        "  def __init__(self,my_pairs):\n",
        "    super().__init__()\n",
        "    self.my_pairs = my_pairs\n",
        "    self.sample_len = len(my_pairs)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.sample_len\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    index = min(max(0,index),self.sample_len-1)\n",
        "\n",
        "    x = self.my_pairs[index][0]\n",
        "    y = self.my_pairs[index][1]\n",
        "\n",
        "    #文本索引张量化，给后续的embedding层处理\n",
        "    x = [english_word2index[word] for word in x.split(\" \")]\n",
        "    x.append(EOS_TOKEN)\n",
        "    tensor_x = torch.tensor(x,dtype=torch.long).to(device)\n",
        "\n",
        "    y = [french_word2index[word] for word in y.split(\" \")]\n",
        "    y.append(EOS_TOKEN)\n",
        "    tensor_y = torch.tensor(y,dtype=torch.long).to(device)\n",
        "    return tensor_x,tensor_y"
      ],
      "metadata": {
        "id": "0DI0wEHjvnPm",
        "cellView": "form"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def use_dataset():\n",
        "  my_dataset = SeqDataset(my_pairs)\n",
        "  my_dataloader = DataLoader(dataset=my_dataset,batch_size=1,shuffle=True)\n",
        "  return my_dataloader"
      ],
      "metadata": {
        "id": "Y7zssM4L3OfR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dataloader = use_dataset()"
      ],
      "metadata": {
        "id": "B0Qq3UFH5vGZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 构建基于GRU的编码器\n",
        "class EncodeGru(nn.Module):\n",
        "  def __init__(self,vocb_size,hidden_size):\n",
        "    super().__init__()\n",
        "    self.vocb_size = vocb_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    #将输入进embedding词嵌入层转换成词向量\n",
        "    self.embed = nn.Embedding(vocb_size,hidden_size).to(device)\n",
        "\n",
        "    #实例化GRU层\n",
        "    self.gru = nn.GRU(hidden_size,hidden_size,batch_first=True).to(device)\n",
        "\n",
        "  def forward(self,vocb_size,hidden):\n",
        "    #数据经过词嵌入层\n",
        "    output = self.embed(vocb_size)\n",
        "    output,hidden = self.gru(output,hidden)\n",
        "    return output,hidden\n",
        "\n",
        "  def inithidden(self):\n",
        "    return torch.zeros(1,1,self.hidden_size).to(device)"
      ],
      "metadata": {
        "id": "JyH88sxPTJbB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocb_size = english_word_n\n",
        "hidden_size = 256\n",
        "encoder = EncodeGru(vocb_size,hidden_size)\n",
        "for x,y in my_dataloader:\n",
        "  hidden = encoder.inithidden()\n",
        "  output,hidden = encoder(x,hidden)\n",
        "  print(output.shape)\n",
        "  print(hidden.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc-jvMXNQEsZ",
        "outputId": "d401d56f-650d-4f63-8fa7-6128387140a4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 9, 256])\n",
            "torch.Size([1, 1, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 构建基于GRU的解码器（不带注意力机制）\n",
        "class DecodeGruN(nn.Module):\n",
        "  def __init__(self,vocab_size,hidden_size):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    #将输入进embedding词嵌入层转换成词向量\n",
        "    self.embedding = nn.Embedding(vocb_size,hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size,hidden_size,batch_first=True)\n",
        "\n",
        "    self.out = nn.Linear(hidden_size,vocab_size)\n",
        "    self.softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "  def forward(self,input,h0):\n",
        "    #input[1,1]\n",
        "    output = self.embedding(input)\n",
        "    #input[1,1,256]\n",
        "    output = F.relu(output)\n",
        "    output,hn = self.gru(output,h0)\n",
        "    #input[1,1,256]\n",
        "    output = self.out(output[0])\n",
        "    #最终输出[1,fre_word_n]\n",
        "    return self.softmax(output),hn\n",
        "\n",
        "  def inithidden(self):\n",
        "    return torch.zeros(1,1,self.hidden_size)"
      ],
      "metadata": {
        "id": "XjZ4OehKc0de",
        "cellView": "form"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = french_word_n\n",
        "hidden_size = 256\n",
        "degrun = DecodeGruN(vocab_size,hidden_size)\n",
        "degrun"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6jtBIHDI4Xz",
        "outputId": "63c5a947-7c5c-47ff-9ffd-131414f51e5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecodeGruN(\n",
              "  (embedding): Embedding(2803, 256)\n",
              "  (gru): GRU(256, 256, batch_first=True)\n",
              "  (out): Linear(in_features=256, out_features=4345, bias=True)\n",
              "  (softmax): LogSoftmax(dim=-1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_demo():\n",
        "  #实例化 数据集对象\n",
        "  #实例化 数据加载器对象\n",
        "  #实例化 编码器对象\n",
        "  encoder = EncodeGru(english_word_n,hidden_size)\n",
        "  #实例化 解码器对象\n",
        "  decoder = DecodeGruN(french_word_n,hidden_size)\n",
        "  for x,y in my_dataloader:\n",
        "    #数据投入编码器\n",
        "    encoder_output,encoder_hidden = encoder(x,encoder.inithidden())\n",
        "    print(encoder_output.shape)\n",
        "    #编码器的结果投入解码器\n",
        "    hidden = encoder_hidden\n",
        "    for i in range(y.shape[1]):\n",
        "      temp_vec = y[0][i].reshape(1,-1)\n",
        "      output,hidden = decoder(temp_vec,hidden)\n",
        "      print(output.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "JScat_jNUXS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 构建基于GRU的解码器（带注意力机制）\n",
        "class DecodeGruAtten(nn.Module):\n",
        "  def __init__(self,fre_vocab_size,hidden_size,p=0.1,max_len=MAX_LENGTH):\n",
        "    super().__init__()\n",
        "    self.fre_vocab_size = fre_vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.p = p\n",
        "    self.max_len = max_len\n",
        "\n",
        "    self.embed = nn.Embedding(fre_vocab_size,hidden_size).to(device)\n",
        "    self.drop = nn.Dropout(p=p).to(device)\n",
        "    # 注意力机制的线性层，输入为连接后的嵌入输入和隐藏状态，输出为注意力权重\n",
        "    # 输入形状: (batch_size, hidden_size + hidden_size)\n",
        "    # 输出形状: (batch_size, max_len) - 在编码器输出上的注意力权重\n",
        "    self.attn = nn.Linear(hidden_size + hidden_size,max_len).to(device)\n",
        "\n",
        "    # 线性层用于结合注意力上下文向量和嵌入输入\n",
        "    # 输入形状: (batch_size, 1, hidden_size + hidden_size)\n",
        "    # 输出形状: (batch_size, 1, hidden_size)\n",
        "    self.attn_combine = nn.Linear(hidden_size + hidden_size,hidden_size).to(device)\n",
        "    self.gru = nn.GRU(hidden_size,hidden_size,batch_first=True).to(device)\n",
        "    self.out = nn.Linear(hidden_size,fre_vocab_size).to(device)\n",
        "    self.softmax = nn.LogSoftmax(dim=-1).to(device)\n",
        "\n",
        "  def forward(self,input,hidden,encoder_output):\n",
        "    # input shape: (batch_size, 1)\n",
        "    # hidden shape: (1, batch_size, hidden_size) - 来自 GRU，batch_first=True，num_layers=1\n",
        "    # encoder_output shape: (batch_size, MAX_LENGTH, hidden_size) - 填充后的编码器输出\n",
        "\n",
        "    # 嵌入输入并应用 dropout\n",
        "    embed_y1 = self.embed(input) # shape: (batch_size, 1, hidden_size)\n",
        "    embed_y2 = self.drop(embed_y1) # shape: (batch_size, 1, hidden_size)\n",
        "\n",
        "    # 计算注意力权重\n",
        "    # 连接嵌入输入（压缩）和隐藏状态（压缩）\n",
        "    # embed_y2_squeezed shape: (batch_size, hidden_size)\n",
        "    # hidden_squeezed shape: (batch_size, hidden_size)\n",
        "    # 连接后的形状: (batch_size, hidden_size + hidden_size)\n",
        "    attn_weights = F.softmax(self.attn(torch.cat((embed_y2.squeeze(0),hidden.squeeze(0)),dim=-1)),dim=-1) # shape: (batch_size, MAX_LENGTH)\n",
        "\n",
        "    atten1 = torch.bmm(attn_weights.unsqueeze(dim=0), encoder_output.unsqueeze(dim=0))\n",
        "        # 5.需要将embed_y1和atten1需要再次拼接:temp_vec-->[1, 512]\n",
        "    temp_vec = torch.cat((embed_y1[0], atten1[0]), dim=-1)\n",
        "    # 6. 将拼接后的结果进行线性变换,按照指定尺寸输出:combin_output-->[1,1, 256]\n",
        "    combin_output = self.attn_combine(temp_vec).unsqueeze(dim=0)\n",
        "    # 7. 将上述结果经过relu:relu_output-->[1,1,256]\n",
        "    relu_output = F.relu(combin_output)\n",
        "\n",
        "    # 通过 GRU\n",
        "    # gru_output shape: (batch_size, 1, hidden_size)\n",
        "    # hn shape: (1, batch_size, hidden_size)\n",
        "    gru_output,hn = self.gru(relu_output,hidden)\n",
        "\n",
        "    # 最终的线性层和 softmax，用于输出概率\n",
        "    # result shape: (batch_size, fre_vocab_size)\n",
        "    result = self.out(gru_output[0]) # 在线性层之前压缩时间步维度\n",
        "    output = self.softmax(result) # shape: (batch_size, fre_vocab_size)\n",
        "\n",
        "    return output,hn,attn_weights"
      ],
      "metadata": {
        "id": "baK_S7-DG5KX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demo_test_att():\n",
        "  #实例化 数据集对象\n",
        "  #实例化 数据加载器对象\n",
        "  #实例化 编码器对象\n",
        "  encoder = EncodeGru(english_word_n,hidden_size).to(device)\n",
        "  #实例化 解码器对象\n",
        "  atten_decoder = DecodeGruAtten(french_word_n,hidden_size).to(device)\n",
        "  for x,y in my_dataloader:\n",
        "    #数据投入编码器\n",
        "    encoder_output,encoder_hidden = encoder(x,encoder.inithidden())\n",
        "    print(encoder_output.shape)\n",
        "    # Ensure encoder_output_c is on the same device as other tensors and has a batch dimension\n",
        "    encoder_output_c = torch.zeros(MAX_LENGTH,hidden_size).to(device)\n",
        "    for i in range(encoder_output.shape[1]):\n",
        "      encoder_output_c[i] = encoder_output[0,i]\n",
        "    # Add batch dimension\n",
        "    encoder_output_c = encoder_output_c.unsqueeze(0)\n",
        "\n",
        "    #编码器的结果投入解码器\n",
        "    for i in range(y.shape[1]):\n",
        "      temp_vec = y[0][i].reshape(1,-1).to(device)\n",
        "      output,hidden,atten_weights = atten_decoder(temp_vec,encoder_hidden,encoder_output_c)\n",
        "      print(output.shape)\n",
        "      # print(encoder_output_c) # This print statement might produce a large output, commenting out for now\n",
        "\n",
        "    break\n",
        "demo_test_att()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhEhLimblmOY",
        "outputId": "21c6007f-d1dc-4d22-cf71-c9f325142305",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 9, 256])\n",
            "torch.Size([1, 4345])\n",
            "torch.Size([1, 4345])\n",
            "torch.Size([1, 4345])\n",
            "torch.Size([1, 4345])\n",
            "torch.Size([1, 4345])\n",
            "torch.Size([1, 4345])\n",
            "torch.Size([1, 4345])\n",
            "torch.Size([1, 4345])\n",
            "torch.Size([1, 4345])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 构建模型训练函数\n",
        "from tqdm import tqdm\n",
        "lr = 1e-4\n",
        "epochs = 2\n",
        "teach_forcing_ratio = 0.5\n",
        "hidden_size = 255\n",
        "\n",
        "def train_model():\n",
        "  my_data = SeqDataset(my_pairs=my_pairs)\n",
        "  my_dataloader = DataLoader(dataset=my_data,batch_size=1,shuffle=True)\n",
        "  encoder = EncodeGru(english_word_n,hidden_size).to(device)\n",
        "  decoder = DecodeGruAtten(french_word_n,hidden_size).to(device)\n",
        "  plot_loss_list = []\n",
        "\n",
        "  loss_fn = nn.NLLLoss()\n",
        "  encoder_optimizer = optim.Adam(encoder.parameters(),lr=lr)\n",
        "  decoder_optimizer = optim.Adam(decoder.parameters(),lr=lr)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    starttime = time.time()\n",
        "    print_loss_total,plot_loss_total = 0.0,0.0\n",
        "    for i,(x,y) in tqdm(enumerate(my_dataloader,start=1)):\n",
        "      loss = train_teacher_forcing(x,y,encoder,decoder,loss_fn,encoder_optimizer,decoder_optimizer)\n",
        "      print_loss_total += loss\n",
        "      plot_loss_total += loss\n",
        "      #每1000轮打印一次损失\n",
        "      if i % 1000 == 0:\n",
        "        print_loss_avg = print_loss_total/1000\n",
        "        print_loss_total = 0.0\n",
        "        #打印轮次，平均损失，所用训练时间\n",
        "        print('当前训练的轮次为：%s, 平均损失为：%s, 训练耗时：%s' % (epoch+1, print_loss_avg, time.time()-starttime))\n",
        "      if i % 100 == 0:\n",
        "        plot_loss_list.append(plot_loss_total/100)\n",
        "        plot_loss_total = 0.0\n",
        "\n",
        "      #保存模型\n",
        "      torch.save(encoder.state_dict(),\"/content/drive/MyDrive/NLP/model/encoder.pth\")\n",
        "      torch.save(decoder.state_dict(),\"/content/drive/MyDrive/NLP/model/decoder.pth\")\n",
        "  plt.figure()\n",
        "  plt.plot(plot_loss_list)\n",
        "  plt.savefig(\"/content/drive/MyDrive/NLP/model/loss.png\")\n",
        "  plt.show()\n",
        "\n",
        "def train_teacher_forcing(x,y,encoder,decoder,loss_fn,encoder_optimizer,decoder_optimizer):\n",
        "  encoder_hidden = encoder.inithidden()\n",
        "  encoder_output,encoder_hidden = encoder(x,encoder_hidden)\n",
        "\n",
        "  encoder_output_padded = torch.zeros(encoder_output.shape[0], MAX_LENGTH, hidden_size).to(device)\n",
        "  seq_len = encoder_output.shape[1]\n",
        "  encoder_output_padded[:, :seq_len, :] = encoder_output[:, :seq_len, :]\n",
        "\n",
        "  decoder_hidden = encoder_hidden\n",
        "  input_y = torch.tensor([[SOS_TOKEN]],dtype=torch.long).to(device)\n",
        "\n",
        "  my_loss = 0.0\n",
        "  y_len = y.shape[1]\n",
        "  use_teacher_forcing = 1 if random.random() < teach_forcing_ratio else 0\n",
        "  if use_teacher_forcing:\n",
        "    for i in range(y_len):\n",
        "      output_y,decoder_hidden,atten_weights = decoder(input_y,decoder_hidden,encoder_output_padded)\n",
        "      target_y = y[0][i].view(1)\n",
        "      my_loss = my_loss + loss_fn(output_y,target_y)\n",
        "      input_y = y[0][i].view(1,-1)\n",
        "  else:\n",
        "    for i in range(y_len):\n",
        "      output_y,decoder_hidden,atten_weights = decoder(input_y,decoder_hidden,encoder_output_padded)\n",
        "      target_y = y[0][i].view(1)\n",
        "      my_loss = my_loss + loss_fn(output_y,target_y)\n",
        "      topv,topi = output_y.topk(1)\n",
        "      if topi.squeeze(1) == EOS_TOKEN:\n",
        "        break\n",
        "      input_y = topi.detach()\n",
        "  #梯度清零\n",
        "  encoder_optimizer.zero_grad()\n",
        "  decoder_optimizer.zero_grad()\n",
        "  #反向传播\n",
        "  my_loss.backward()\n",
        "  #梯度更新\n",
        "  encoder_optimizer.step()\n",
        "  decoder_optimizer.step()\n",
        "\n",
        "  return my_loss.item()/y_len\n",
        "\n",
        "train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 914
        },
        "id": "t1nGn0dkT1Vd",
        "outputId": "2e2dd078-76be-42ed-9f4d-df679f7a17df",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1002it [02:04, 10.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：4.021194262384987, 训练耗时：124.11559915542603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2001it [04:09,  7.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：3.661198564737186, 训练耗时：249.38450646400452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3001it [06:13,  9.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：3.4523617556923916, 训练耗时：373.6092622280121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4001it [08:22,  9.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：3.3079223645104294, 训练耗时：502.3656086921692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5000it [10:37,  7.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：3.2856225761807147, 训练耗时：636.993305683136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "6000it [12:57,  9.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：3.191547954574467, 训练耗时：777.1756322383881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "6999it [15:09,  9.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：3.0775495722589077, 训练耗时：909.0458691120148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "8000it [17:17,  6.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：3.0212512742897832, 训练耗时：1037.6473650932312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9001it [19:26,  7.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.9832370890288167, 训练耗时：1166.2478213310242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10001it [21:33,  9.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.942836958563135, 训练耗时：1293.8016946315765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11001it [23:41,  8.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.9205502190726116, 训练耗时：1421.7534267902374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12001it [25:50,  9.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.8871494023039235, 训练耗时：1550.272677898407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "13001it [27:57,  9.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.8372564709046526, 训练耗时：1677.4068076610565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "14001it [30:05,  9.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.8501881523777546, 训练耗时：1805.7273972034454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "15001it [32:12,  9.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.800535208777398, 训练耗时：1932.3646359443665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "16000it [34:18,  8.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.7307099197928872, 训练耗时：2058.0349972248077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "17000it [36:23,  9.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.714087877810189, 训练耗时：2183.552980899811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "17998it [38:29,  9.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.6221546267399747, 训练耗时：2309.1070840358734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "19001it [40:35,  7.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.6732841259460582, 训练耗时：2435.546188354492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20002it [42:42,  7.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.5828984836008813, 训练耗时：2562.0197076797485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "21001it [44:48,  8.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.5789597879737154, 训练耗时：2688.3246603012085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "22001it [46:54,  7.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.610603003252497, 训练耗时：2814.1969015598297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "23002it [49:01,  8.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.593225075268178, 训练耗时：2941.3684265613556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "24000it [51:06,  6.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.483364989281174, 训练耗时：3066.7591784000397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "25001it [53:12,  8.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.4386004473289793, 训练耗时：3191.9943459033966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "26002it [55:18,  8.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.4571577307834542, 训练耗时：3318.323675632477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "27001it [57:25,  8.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.470478063972598, 训练耗时：3445.0041592121124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "28001it [59:42,  7.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.4789369333367466, 训练耗时：3581.890130996704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "29001it [1:01:57, 11.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.3592175031671396, 训练耗时：3717.6590180397034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "30001it [1:04:11, 10.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.3522247069501665, 训练耗时：3851.6611733436584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "31000it [1:06:20,  9.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前训练的轮次为：0, 平均损失为：2.4023057767854796, 训练耗时：3980.945950984955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "31411it [1:07:12,  7.79it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-19-3105762911.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0my_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-19-3105762911.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0;31m#保存模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/NLP/model/encoder.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m       \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/NLP/model/decoder.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_loss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 构建模型评估函数\n",
        "encoder_path = \"/content/drive/MyDrive/NLP/model/encoder_1.pth\"\n",
        "decoder_path = \"/content/drive/MyDrive/NLP/model/decoder_1.pth\"\n",
        "def eval_model():\n",
        "  hidden_size = 256 # Correct hidden size to match the saved model\n",
        "  encoder_model = EncodeGru(english_word_n,hidden_size)\n",
        "  decoder_model = DecodeGruAtten(french_word_n,hidden_size)\n",
        "  encoder_model.load_state_dict(torch.load(encoder_path,map_location=\"cpu\"))\n",
        "  decoder_model.load_state_dict(torch.load(decoder_path,map_location=\"cpu\"))\n",
        "  print(encoder_model)\n",
        "  print(decoder_model)\n",
        "  my_pair = [\n",
        "      ['i m .', 'j ai ans .'],\n",
        "      ['i m ok .','je vais bien .'],\n",
        "      ['i m ok .',      'ca va .'],\n",
        "      ['i m fat .',     'je suis gras .'],\n",
        "      ['i m fat .',     'je suis gros .']\n",
        "  ]\n",
        "  for i,pair in enumerate(my_pair):\n",
        "    x = pair[0]\n",
        "    y = pair[1]\n",
        "    x_encoder =[english_word2index[i] for i in x.split(\" \")]\n",
        "    x_encoder.append(EOS_TOKEN)\n",
        "    x_tensor = torch.tensor(x_encoder,dtype=torch.long).view(1,-1)\n",
        "    decoder_word ,atten_weights = predict_word(x_tensor,encoder_model,decoder_model)\n",
        "    output_word = \" \".join(decoder_word)\n",
        "    print(f\"x--->{x}\")\n",
        "    print(f\"y--->{y}\")\n",
        "    print(f'predict--->{output_word}')\n",
        "\n",
        "\n",
        "def predict_word(x_tensor,encoder_model,decoder_model):\n",
        "  with torch.no_grad():\n",
        "    #数据经过编码器处理\n",
        "    encoder_output,encoder_hidden = encoder_model(x_tensor,encoder_model.inithidden())\n",
        "\n",
        "    encoder_output_c = torch.zeros(MAX_LENGTH,encoder_model.hidden_size)\n",
        "    x_len = x_tensor.shape[1] # Use x_tensor here instead of x\n",
        "    for i in range(x_len):\n",
        "      encoder_output_c[i] = encoder_output[0,i]\n",
        "\n",
        "    #解码\n",
        "    decoder_hidden = encoder_hidden\n",
        "    input_y = torch.tensor([[SOS_TOKEN]],dtype=torch.long)\n",
        "    decoder_word = []\n",
        "\n",
        "    decoder_atten = torch.zeros(MAX_LENGTH,MAX_LENGTH)\n",
        "    for i in range(MAX_LENGTH):\n",
        "      output_y,decoder_hidden,atten_weights = decoder_model(input_y,decoder_hidden,encoder_output_c)\n",
        "      topv,topi = torch.topk(output_y,k=1)\n",
        "      decoder_atten[i] = atten_weights\n",
        "      if topi.item() == EOS_TOKEN:\n",
        "        break\n",
        "      else:\n",
        "        decoder_word.append(french_index2word[topi.item()])\n",
        "      input_y = topi\n",
        "  return decoder_word ,decoder_atten[:i+1]\n",
        "\n",
        "eval_model()"
      ],
      "metadata": {
        "id": "YmBeoJsDTJxs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46899e9b-a176-4833-8e3a-f8a6e23f0575"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EncodeGru(\n",
            "  (embed): Embedding(2803, 256)\n",
            "  (gru): GRU(256, 256, batch_first=True)\n",
            ")\n",
            "DecodeGruAtten(\n",
            "  (embed): Embedding(4345, 256)\n",
            "  (drop): Dropout(p=0.1, inplace=False)\n",
            "  (attn): Linear(in_features=512, out_features=10, bias=True)\n",
            "  (attn_combine): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (gru): GRU(256, 256, batch_first=True)\n",
            "  (out): Linear(in_features=256, out_features=4345, bias=True)\n",
            "  (softmax): LogSoftmax(dim=-1)\n",
            ")\n",
            "x--->i m .\n",
            "y--->j ai ans .\n",
            "predict--->je suis .\n",
            "x--->i m ok .\n",
            "y--->je vais bien .\n",
            "predict--->je vais .\n",
            "x--->i m ok .\n",
            "y--->ca va .\n",
            "predict--->je vais .\n",
            "x--->i m fat .\n",
            "y--->je suis gras .\n",
            "predict--->je suis impressionnee .\n",
            "x--->i m fat .\n",
            "y--->je suis gros .\n",
            "predict--->je suis impressionnee .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 构建模型测试函数\n",
        "\n",
        "#@title 构建模型评估函数\n",
        "encoder_path = \"/content/drive/MyDrive/NLP/model/encoder_1.pth\"\n",
        "decoder_path = \"/content/drive/MyDrive/NLP/model/decoder_1.pth\"\n",
        "def eval_model():\n",
        "  hidden_size = 256 # Correct hidden size to match the saved model\n",
        "  encoder_model = EncodeGru(english_word_n,hidden_size)\n",
        "  decoder_model = DecodeGruAtten(french_word_n,hidden_size)\n",
        "  encoder_model.load_state_dict(torch.load(encoder_path,map_location=\"cpu\"))\n",
        "  decoder_model.load_state_dict(torch.load(decoder_path,map_location=\"cpu\"))\n",
        "  print(encoder_model)\n",
        "  print(decoder_model)\n",
        "  my_pair = [\n",
        "      ['i love you .'],\n",
        "  ]\n",
        "  for i,pair in enumerate(my_pair):\n",
        "    x = pair\n",
        "    x_encoder =[english_word2index[i] for i in pair[0].split(\" \")]\n",
        "    x_encoder.append(EOS_TOKEN)\n",
        "    x_tensor = torch.tensor(x_encoder,dtype=torch.long).view(1,-1)\n",
        "    decoder_word ,atten_weights = predict_word(x_tensor,encoder_model,decoder_model)\n",
        "    output_word = \" \".join(decoder_word)\n",
        "    print(f\"x--->{x}\")\n",
        "    print(f'predict--->{output_word}')\n",
        "\n",
        "\n",
        "def predict_word(x_tensor,encoder_model,decoder_model):\n",
        "  with torch.no_grad():\n",
        "    #数据经过编码器处理\n",
        "    encoder_output,encoder_hidden = encoder_model(x_tensor,encoder_model.inithidden())\n",
        "\n",
        "    encoder_output_c = torch.zeros(MAX_LENGTH,encoder_model.hidden_size)\n",
        "    x_len = x_tensor.shape[1] # Use x_tensor here instead of x\n",
        "    for i in range(x_len):\n",
        "      encoder_output_c[i] = encoder_output[0,i]\n",
        "\n",
        "    #解码\n",
        "    decoder_hidden = encoder_hidden\n",
        "    input_y = torch.tensor([[SOS_TOKEN]],dtype=torch.long)\n",
        "    decoder_word = []\n",
        "\n",
        "    decoder_atten = torch.zeros(MAX_LENGTH,MAX_LENGTH)\n",
        "    for i in range(MAX_LENGTH):\n",
        "      output_y,decoder_hidden,atten_weights = decoder_model(input_y,decoder_hidden,encoder_output_c)\n",
        "      topv,topi = torch.topk(output_y,k=1)\n",
        "      decoder_atten[i] = atten_weights\n",
        "      if topi.item() == EOS_TOKEN:\n",
        "        break\n",
        "      else:\n",
        "        decoder_word.append(french_index2word[topi.item()])\n",
        "      input_y = topi\n",
        "  return decoder_word ,decoder_atten[:i+1]\n",
        "\n",
        "eval_model()"
      ],
      "metadata": {
        "id": "PaivcV_dcyTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "715b4096-486b-4859-b9e4-033c0d8ca29c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EncodeGru(\n",
            "  (embed): Embedding(2803, 256)\n",
            "  (gru): GRU(256, 256, batch_first=True)\n",
            ")\n",
            "DecodeGruAtten(\n",
            "  (embed): Embedding(4345, 256)\n",
            "  (drop): Dropout(p=0.1, inplace=False)\n",
            "  (attn): Linear(in_features=512, out_features=10, bias=True)\n",
            "  (attn_combine): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (gru): GRU(256, 256, batch_first=True)\n",
            "  (out): Linear(in_features=256, out_features=4345, bias=True)\n",
            "  (softmax): LogSoftmax(dim=-1)\n",
            ")\n",
            "x--->['i love you .']\n",
            "predict--->je vous vous .\n"
          ]
        }
      ]
    }
  ]
}