{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkpMqxJqXSoVrDlbeTkYwo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluetinue/Country_Name/blob/main/%E5%9F%BA%E4%BA%8EGRU%E7%9A%84seq2seq%E7%9A%84%E8%8B%B1%E8%AF%91%E6%B3%95%E6%A1%88%E4%BE%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "9s4U2eD2SVwe",
        "outputId": "fbf2a3a1-285a-4963-a8a3-3af69128514e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@title 导包\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 用于正则表达式\n",
        "import re\n",
        "# 用于构建网络结构和函数的torch工具包\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# torch中预定义的优化方法工具包\n",
        "import torch.optim as optim\n",
        "import time\n",
        "# 用于随机生成数据\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 全局变量\n",
        "\n",
        "#开始字符标注\n",
        "SOS_TOKEN = 0\n",
        "EOS_TOKEN = 1\n",
        "\n",
        "#最大句子长度\n",
        "MAX_LENGTH = 10\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/NLP/data/eng-fra-v2.txt\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "mE9rJIqSbVHp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 文本清洗工具函数\n",
        "def normal2String(s):\n",
        "  s1 = s.lower().strip()\n",
        "  s2 = re.sub(r\"([.!?])\", r\" \\1 \", s1)\n",
        "  s3 = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s2)\n",
        "  return s3.strip()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1v6xnFkLTJYd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 对原始数据进行预处理\n",
        "#构建出[[英文,法文，.....的列表对]]\n",
        "def wash_data():\n",
        "  with open(data_path,\"r\",encoding=\"utf-8\") as fr:\n",
        "    lines = fr.readlines()\n",
        "  #列表推导式\n",
        "  my_pairs = [[ normal2String(s) for s in i.strip().split(\"\\t\")] for i in lines]\n",
        "\n",
        "  #初始化词表，默认有开始和结束分隔符和初始长度\n",
        "  english_word2index = {\"SOS\":0,\"EOS\":1}\n",
        "  english_word2index_n = 2\n",
        "  fres_word2index = {\"SOS\":0,\"EOS\":1}\n",
        "  fres_word2index_n = 2\n",
        "\n",
        "  #构造两个词表的word2index表\n",
        "  for pair in my_pairs:\n",
        "    for word in pair[0].split(\" \"):\n",
        "      if word not in english_word2index:\n",
        "        english_word2index[word] = len(english_word2index)\n",
        "        #english_word2index[word] = english_word2index_n\n",
        "        #english_word2index_n += 1\n",
        "\n",
        "    for word in pair[1].split(\" \"):\n",
        "      if word not in fres_word2index:\n",
        "        fres_word2index[word] = len(fres_word2index)\n",
        "        #fres_word2index[word] = fres_word2index_n\n",
        "        #fres_word2index_n += 1\n",
        "\n",
        "  #构造两个词表的index2word表\n",
        "  english_index2word = {v:k for k,v in english_word2index.items()}\n",
        "  fres_index2word = {v:k for k,v in fres_word2index.items()}\n",
        "  return english_word2index,english_index2word,\\\n",
        "    len(english_word2index),fres_word2index,fres_index2word,\\\n",
        "    len(fres_index2word),my_pairs"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1auaNNPuc5OS"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_word2index, english_index2word,  english_word_n, french_word2index, french_index2word, french_word_n, my_pairs = wash_data()"
      ],
      "metadata": {
        "id": "9K1cxBUSvDxZ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 构建数据源对象\n",
        "class SeqDataset(Dataset):\n",
        "  def __init__(self,my_pairs):\n",
        "    super().__init__()\n",
        "    self.my_pairs = my_pairs\n",
        "    self.sample_len = len(my_pairs)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.sample_len\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    index = min(max(0,index),self.sample_len-1)\n",
        "\n",
        "    x = self.my_pairs[index][0]\n",
        "    y = self.my_pairs[index][1]\n",
        "\n",
        "    #文本索引张量化，给后续的embedding层处理\n",
        "    x = [english_word2index[word] for word in x.split(\" \")]\n",
        "    x.append(EOS_TOKEN)\n",
        "    tensor_x = torch.tensor(x,dtype=torch.long)\n",
        "\n",
        "    y = [french_word2index[word] for word in y.split(\" \")]\n",
        "    y.append(EOS_TOKEN)\n",
        "    tensor_y = torch.tensor(y,dtype=torch.long)\n",
        "    return tensor_x,tensor_y\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0DI0wEHjvnPm"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def use_dataset():\n",
        "  my_dataset = SeqDataset(my_pairs)\n",
        "  my_dataloader = DataLoader(dataset=my_dataset,batch_size=1,shuffle=True)\n",
        "  return my_dataloader"
      ],
      "metadata": {
        "id": "Y7zssM4L3OfR"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dataloader = use_dataset()"
      ],
      "metadata": {
        "id": "B0Qq3UFH5vGZ"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 构建基于GRU的编码器\n",
        "class EncodeGru(nn.Module):\n",
        "  def __init__(self,vocb_size,hidden_size):\n",
        "    super().__init__()\n",
        "    self.vocb_size = vocb_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    #将输入进embedding词嵌入层转换成词向量\n",
        "    self.embedding = nn.Embedding(vocb_size,hidden_size)\n",
        "\n",
        "    #实例化GRU层\n",
        "    self.gru = nn.GRU(hidden_size,hidden_size,batch_first=True)\n",
        "\n",
        "  def forward(self,vocb_size,hidden):\n",
        "    #数据经过词嵌入层\n",
        "    output = self.embedding(vocb_size)\n",
        "    output,hidden = self.gru(output,hidden)\n",
        "    return output,hidden\n",
        "\n",
        "  def inithidden(self):\n",
        "    return torch.zeros(1,1,self.hidden_size)"
      ],
      "metadata": {
        "id": "JyH88sxPTJbB"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocb_size = english_word_n\n",
        "hidden_size = 256\n",
        "encoder = EncodeGru(vocb_size,hidden_size)\n",
        "for x,y in my_dataloader:\n",
        "  hidden = encoder.inithidden()\n",
        "  output,hidden = encoder(x,hidden)\n",
        "  print(output.shape)\n",
        "  print(hidden.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc-jvMXNQEsZ",
        "outputId": "44474f53-c2b8-4d91-8013-fab76468177e"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 6, 256])\n",
            "torch.Size([1, 1, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 构建基于GRU的解码器"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XjZ4OehKc0de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 构建模型评估函数"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YmBeoJsDTJxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 构建模型测试函数"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PaivcV_dcyTV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}